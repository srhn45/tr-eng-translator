{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the data distribution\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_token_lengths(filepath, chunksize=100_000, max_tokens=50):\n",
    "    counter = Counter()\n",
    "    total = 0\n",
    "\n",
    "    for chunk in pd.read_csv(filepath, usecols=[\"input_ids\"], chunksize=chunksize):\n",
    "        lengths = chunk[\"input_ids\"].str.count(\" \") + 1\n",
    "        lengths = lengths[lengths <= max_tokens]  # Filter out long sequences\n",
    "        counter.update(lengths)\n",
    "        total += len(chunk)\n",
    "\n",
    "    print(f\"\\nFinal counts by token length (‚â§ {max_tokens}):\")\n",
    "    for length in sorted(counter):\n",
    "        print(f\"Length {length}: {counter[length]:,}\")\n",
    "\n",
    "    lengths = list(counter.keys())\n",
    "    counts = list(counter.values())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(lengths, counts)\n",
    "    plt.xlabel(\"Token Length\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.title(f\"Distribution of Token Lengths ‚â§ {max_tokens}\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "count_token_lengths(\"../data/tokenized_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_token_lengths(\"../data/tokenized_data_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import tqdm\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import os, requests, json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import sentencepiece as spm\n",
    "import random\n",
    "import sacrebleu\n",
    "#import multiprocessing\n",
    "#from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  \n",
    "\n",
    "WEBHOOK = os.getenv(\"DISCORD_WEBHOOK_URL\")\n",
    "if not WEBHOOK:\n",
    "    raise RuntimeError(\"Please create a .env with DISCORD_WEBHOOK_URL\") # i didn't want to bother making it optional, sorry\n",
    "TELEGRAM_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "TELEGRAM_CHAT_ID = os.getenv(\"TELEGRAM_CHAT_ID\")\n",
    "if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:\n",
    "    raise RuntimeError(\"Please create a .env with TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID\") # same as above\n",
    "\n",
    "def send_discord(content: str=None, embed: dict=None, tqdm_obj=None): # to track training progress on my phone when i'm away from my computer\n",
    "    if tqdm_obj is not None:\n",
    "        processed = tqdm_obj.n\n",
    "        total = tqdm_obj.total\n",
    "        time = tqdm_obj.format_dict[\"elapsed\"]\n",
    "\n",
    "        if total > 0:\n",
    "            percentage = (processed / total) * 100\n",
    "        else:\n",
    "            percentage = 0\n",
    "\n",
    "        content = f\"Progress: {processed}/{total} ({percentage:.2f}%) \\n Elapsed time: {time/60:.1f}s \\n Rate: {processed / time:.2f} items/s\"\n",
    "\n",
    "    payload = {}\n",
    "    if content:\n",
    "        payload[\"content\"] = content\n",
    "    if embed:\n",
    "        payload[\"embeds\"] = [embed]\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(WEBHOOK,\n",
    "                            data=json.dumps(payload),\n",
    "                            headers=headers,\n",
    "                            timeout=5)\n",
    "        resp.raise_for_status() # check for HTTP errors\n",
    "    except Exception as e:\n",
    "        pass # since discord is banned in turkey (yay), i just want this to fail silently for now. maybe one day it'll start working again..\n",
    "\n",
    "\n",
    "def send_telegram(content: str = None, embed: dict = None, tqdm_obj=None):\n",
    "    if tqdm_obj is not None:\n",
    "        processed = tqdm_obj.n\n",
    "        total = tqdm_obj.total\n",
    "        time = tqdm_obj.format_dict[\"elapsed\"]\n",
    "\n",
    "        if total > 0:\n",
    "            percentage = (processed / total) * 100\n",
    "        else:\n",
    "            percentage = 0\n",
    "\n",
    "        content = f\"Progress: {processed}/{total} ({percentage:.2f}%) \\n Elapsed time: {time/60:.1f}m \\n Rate: {processed / time:.2f} items/s\"\n",
    "\n",
    "    if embed:\n",
    "        lines = []\n",
    "        if 'title' in embed:\n",
    "            lines.append(f\"<b>{embed['title']}</b>\")\n",
    "\n",
    "        for field in embed.get(\"fields\", []):\n",
    "            name = field.get(\"name\", \"\")\n",
    "            value = field.get(\"value\", \"\")\n",
    "            lines.append(f\"<b>{name}:</b> {value}\")\n",
    "\n",
    "        if content:\n",
    "            lines.append(\"\")  # add a newline\n",
    "            lines.append(content)\n",
    "\n",
    "        content = \"\\n\".join(lines)\n",
    "\n",
    "    if not content:\n",
    "        return  # nothing to send\n",
    "\n",
    "    url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage\"\n",
    "    payload = {\n",
    "        \"chat_id\": TELEGRAM_CHAT_ID,\n",
    "        \"text\": content,\n",
    "        \"parse_mode\": \"HTML\"\n",
    "    }\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, json=payload, headers=headers, timeout=5)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e_direct:\n",
    "        print(f\"‚ùå Telegram error: {e_direct}\")\n",
    "\n",
    "\n",
    "def send_message(content: str = None, embed: dict = None, tqdm_obj=None):\n",
    "    send_discord(content=content, embed=embed, tqdm_obj=tqdm_obj)\n",
    "    send_telegram(content=content, embed=embed, tqdm_obj=tqdm_obj)\n",
    "\n",
    "\n",
    "def make_embed(epoch, train_loss, val_loss, lr, done_steps, max_tokens, batches_per_size):\n",
    "    e = {\n",
    "      \"title\": f\"Epoch {epoch} Complete üéâ\",\n",
    "      \"color\": 0x56B4E9,\n",
    "      \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "      \"fields\": [\n",
    "        {\"name\": \"Train Loss\", \"value\": f\"{train_loss:.4f}\", \"inline\": True},\n",
    "        {\"name\": \"Val Loss\",   \"value\": f\"{val_loss:.4f}\",   \"inline\": True},\n",
    "        {\"name\": \"LR\",         \"value\": f\"{lr:.8f}\",         \"inline\": True},\n",
    "        {\"name\": \"Step\",       \"value\": f\"{done_steps}\",     \"inline\": True},\n",
    "        {\"name\": \"Max Tokens\", \"value\": f\"{max_tokens}\",     \"inline\": True},\n",
    "        {\"name\": \"Batches/Size\", \"value\": f\"{batches_per_size}\", \"inline\": True},\n",
    "      ]\n",
    "    }\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high') # for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_fast(s):\n",
    "    return s.str.count(\" \") + 1 # this is lightspeed, gamechanger for me\n",
    "\n",
    "def parse_string(s):\n",
    "    return s.str.split().apply(lambda x: np.array([int(tok) for tok in x if tok.isdigit()], dtype=np.int32))\n",
    "\n",
    "class DatasetFromCSV(Dataset):\n",
    "    def __init__(self, filepath, mode=\"train\", batches_per_size=3000, max_tokens=15, batch_size=16):\n",
    "        \n",
    "        if mode == \"val\":\n",
    "            max_tokens = 40\n",
    "            batches_per_size = 500\n",
    "\n",
    "        df = pd.read_csv(filepath, usecols=[\"input_ids\", \"label_ids\"])\n",
    "        df[\"length\"] = df[\"input_ids\"].str.count(\" \") + 1\n",
    "        df = df[df[\"length\"] <= max_tokens].copy()\n",
    "\n",
    "\n",
    "        df[\"parsed_input_ids\"] = parse_string(df[\"input_ids\"])\n",
    "        df[\"parsed_label_ids\"] = parse_string(df[\"label_ids\"])\n",
    "\n",
    "        max_examples_per_length = batches_per_size * batch_size\n",
    "\n",
    "        selected_input_ids = []\n",
    "        selected_label_ids = []\n",
    "\n",
    "        for length in range(1, max_tokens + 1):\n",
    "            group = df[df[\"length\"] == length]\n",
    "            if len(group) == 0:\n",
    "                continue\n",
    "            sampled = group.sample(n=min(len(group), max_examples_per_length))\n",
    "            selected_input_ids.extend(sampled[\"parsed_input_ids\"].tolist())\n",
    "            selected_label_ids.extend(sampled[\"parsed_label_ids\"].tolist())\n",
    "\n",
    "        self.input_ids = selected_input_ids\n",
    "        self.label_ids = selected_label_ids\n",
    "        \n",
    "\n",
    "        if mode == \"train\":\n",
    "            print(f\"[DatasetFromCSV] Loaded {len(selected_input_ids)} examples up to {max((len(seq) for seq in self.input_ids), default=0)} tokens from {min((len(seq) for seq in self.input_ids), default=0)}.\")\n",
    "            send_message(content=f\"[DatasetFromCSV] Loaded {len(selected_input_ids)} examples up to {max((len(seq) for seq in self.input_ids), default=0)} tokens from {min((len(seq) for seq in self.input_ids), default=0)}.\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "                \"input_ids\": torch.from_numpy(self.input_ids[idx]).long(),\n",
    "                \"label_ids\": torch.from_numpy(self.label_ids[idx]).long()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def get_lengths(self):\n",
    "        return [len(seq) for seq in self.input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, lengths, batch_size=16, shuffle=True, drop_last=True, bucket_size_multiplier=100):\n",
    "        self.lengths = lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.bucket_size = batch_size * bucket_size_multiplier\n",
    "\n",
    "        self.sorted_indices = np.argsort(lengths)\n",
    "        self.batches = self._create_batches()\n",
    "\n",
    "    def _create_batches(self):\n",
    "        batches = []\n",
    "        for i in range(0, len(self.sorted_indices), self.bucket_size):\n",
    "            bucket = self.sorted_indices[i:i+self.bucket_size]\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(bucket)\n",
    "            for j in range(0, len(bucket), self.batch_size):\n",
    "                batch = bucket[j:j+self.batch_size]\n",
    "                if not self.drop_last or len(batch) == self.batch_size:\n",
    "                    batches.append(batch.tolist())\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(batches)\n",
    "        return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, pad_token_id=0, max_len=None):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_len = max_len  # If None, pad to the longest in batch\n",
    "\n",
    "    def _pad_batch(self, batch_sequences):\n",
    "        input_ids = [sample['input_ids'] for sample in batch_sequences]\n",
    "        label_ids = [sample['label_ids'] for sample in batch_sequences]\n",
    "\n",
    "        input_tensor_seqs = [seq.clone().detach() for seq in input_ids]\n",
    "        label_tensor_seqs = [seq.clone().detach() for seq in label_ids]\n",
    "\n",
    "        if self.max_len:\n",
    "            input_tensor_seqs = [seq[:self.max_len] for seq in input_tensor_seqs]\n",
    "            label_tensor_seqs = [seq[:self.max_len] for seq in label_tensor_seqs]\n",
    "        \n",
    "        padded_input = pad_sequence(input_tensor_seqs, batch_first=True, padding_value=self.pad_token_id)\n",
    "        padded_label = pad_sequence(label_tensor_seqs, batch_first=True, padding_value=self.pad_token_id)\n",
    "\n",
    "        if self.max_len and padded_input.size(1) < self.max_len:\n",
    "            pad_size = self.max_len - padded_input.size(1)\n",
    "            pad_tensor = torch.full((padded_input.size(0), pad_size), self.pad_token_id, dtype=torch.long)\n",
    "            padded_input = torch.cat([padded_input, pad_tensor], dim=1)\n",
    "\n",
    "        if self.max_len and padded_label.size(1) < self.max_len:\n",
    "            pad_size = self.max_len - padded_label.size(1)\n",
    "            pad_tensor = torch.full((padded_label.size(0), pad_size), self.pad_token_id, dtype=torch.long)\n",
    "            padded_label = torch.cat([padded_label, pad_tensor], dim=1)\n",
    "\n",
    "        input_key_padding_mask = (padded_input == self.pad_token_id)  # (batch_size, seq_len)\n",
    "        label_key_padding_mask = (padded_label == self.pad_token_id)\n",
    "\n",
    "        return padded_input.contiguous(), input_key_padding_mask.contiguous(), padded_label.contiguous(), label_key_padding_mask.contiguous()\n",
    "\n",
    "    def __call__(self, batch_sequences):\n",
    "        return self._pad_batch(batch_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, pad_token_id=0):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(512, embed_dim) # max length of 100 for positional encoding\n",
    "\n",
    "    def forward(self, input_ids, key_padding_mask): # input_ids: (batch_size, seq_len)\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        token_emb = self.token_embedding(input_ids) * math.sqrt(self.token_embedding.embedding_dim) # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        pos_emb = self.pos_embedding(torch.arange(seq_len, device=input_ids.device)).unsqueeze(0).expand(batch_size, seq_len, -1) # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        emb = token_emb + pos_emb\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(EncodingLayer, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.SiLU(), # using SiLU activation for better performance\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "\n",
    "        x_norm = self.layernorm1(x) # layernorm before attention (pre-norm)\n",
    "        attn_y = self.attn(x_norm, x_norm, x_norm, key_padding_mask=key_padding_mask)[0]\n",
    "        x = x + self.dropout(attn_y)\n",
    "\n",
    "\n",
    "        x_norm = self.layernorm2(x) # using pre-norm for stability\n",
    "        ff_output = self.feedforward(x_norm)\n",
    "        x = x + self.dropout(ff_output)\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodingLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(DecodingLayer, self).__init__()\n",
    "\n",
    "        self.attn1 = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn2 = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.SiLU(), # using SiLU instead of ReLU for better performance  \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_out, tgt_key_padding_mask=None, memory_key_padding_mask=None, causal_mask=None):\n",
    "        if causal_mask is None:\n",
    "            seq_len = x.size(1)\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "\n",
    "        x_norm = self.layernorm1(x) # pre-norm on self attention\n",
    "        masked_attn = self.attn1(x_norm, x_norm, x_norm, attn_mask=causal_mask, key_padding_mask=tgt_key_padding_mask)[0] # masked self attention\n",
    "        x = x + self.dropout(masked_attn)\n",
    "\n",
    "\n",
    "        x_norm = self.layernorm2(x) # pre-norm on cross attention\n",
    "        cross_attn = self.attn2(x_norm, encoder_out, encoder_out, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        x = x + self.dropout(cross_attn)\n",
    "\n",
    "        x_norm = self.layernorm3(x)\n",
    "        ff_output = self.feedforward(x_norm)\n",
    "        x = x + self.dropout(ff_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, ff_dim, num_heads,\n",
    "                 max_seq_len=None, n_encoders=2, n_decoders=2, pad_token_id=0, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.src_embedding = EmbeddingLayer(vocab_size, embed_dim, pad_token_id) # embedding for source\n",
    "        self.tgt_embedding = EmbeddingLayer(vocab_size, embed_dim, pad_token_id) # embedding for target\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([ # stack of encoding layers\n",
    "            EncodingLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(n_encoders)\n",
    "        ])\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([ # stack of decoding layers\n",
    "            DecodingLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(n_decoders)\n",
    "        ])\n",
    "\n",
    "        self.decoder_final_norm = nn.LayerNorm(embed_dim) # final layernorm for decoder output\n",
    "\n",
    "        self.output_projection = nn.Linear(embed_dim, vocab_size) # output layer\n",
    "\n",
    "    def encode(self, src_ids, src_key_padding_mask):\n",
    "        x = self.src_embedding(src_ids, src_key_padding_mask)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "    def decode(self, tgt_ids, encoder_out, tgt_key_padding_mask, memory_key_padding_mask):\n",
    "        x = self.tgt_embedding(tgt_ids, tgt_key_padding_mask)\n",
    "\n",
    "        seq_len = tgt_ids.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=tgt_ids.device), diagonal=1).bool()\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_out, memory_key_padding_mask=memory_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, causal_mask=causal_mask)\n",
    "\n",
    "        x = self.decoder_final_norm(x) # final layernorm for decoder output\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        encoder_out = self.encode(src_ids, src_key_padding_mask)\n",
    "        decoder_out = self.decode(tgt_ids, encoder_out, tgt_key_padding_mask, src_key_padding_mask)\n",
    "        logits = self.output_projection(decoder_out)\n",
    "        return logits  # (batch_size, tgt_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule_with_plateau(warmup_steps=30000, plateau_steps=200000, total_steps=1100000, min_lr_ratio=0.001): # custom cosine schedule with plateau to fit my curriculum learning process\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return current_step / warmup_steps # warmup phase\n",
    "        elif current_step < warmup_steps + plateau_steps:\n",
    "            return 1.0 # plateau phase\n",
    "        else:\n",
    "            progress = (current_step - warmup_steps - plateau_steps) / (total_steps - warmup_steps - plateau_steps)\n",
    "            cosine_decay =  0.5 * (1.0 + math.cos(math.pi * progress)) # decay, cosine annealing\n",
    "            return min_lr_ratio + (1.0 - min_lr_ratio) * cosine_decay # decays down to min_lr_ratio\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, optimizer, preprocessor, load_model_path=None, batch_size=16, device=\"cpu\", save_path=\"best_model.pth\", \n",
    "                 val_filepath=\"../data/tokenized_data_val.csv\", filepath=\"../data/tokenized_data_train.csv\", embed_dim=512,\n",
    "                 max_tokens = 35, batches_per_size=3000,\n",
    "                 num_warmup_steps=33000, num_plateau_steps=200000, num_training_steps=1100000, current_step=0, start_epoch=0):\n",
    "        self.EOS_ID = 3 # this is the EOS token ID in my dataset\n",
    "        self.PAD_ID = 0 # this is the PAD token ID in my dataset\n",
    "        self.BOS_ID = 2 # this is the BOS token ID in my dataset\n",
    "        \n",
    "        self.max_tokens = max_tokens\n",
    "        self.batches_per_size = batches_per_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.filepath=filepath\n",
    "        self.model = model.to(device)\n",
    "        self.start_epoch=start_epoch\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        lr_lambda = cosine_schedule_with_plateau(warmup_steps=num_warmup_steps, plateau_steps=num_plateau_steps, total_steps=num_training_steps)\n",
    "        self.scheduler = LambdaLR(self.optimizer, lr_lambda)\n",
    "\n",
    "        self.current_step = current_step \n",
    "        if current_step > 0:\n",
    "            for _ in range(self.current_step):\n",
    "                self.scheduler.step() # to restart the training without reinitializing the learning rate schedule\n",
    "         \n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.val_filepath = val_filepath\n",
    "\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.epochs_without_improvement = 0\n",
    "\n",
    "        self.scaler = torch.amp.GradScaler(\"cuda\") if device == \"cuda\" else None\n",
    "\n",
    "        if load_model_path:\n",
    "            self.model.load_state_dict(torch.load(load_model_path, map_location=self.device))\n",
    "            print(f\"Model loaded from {load_model_path}\")\n",
    "\n",
    "        self.en_tokenizer = spm.SentencePieceProcessor()\n",
    "        self.tr_tokenizer = spm.SentencePieceProcessor()\n",
    "\n",
    "        self.en_tokenizer.load(\"../data/en_spm.model\")\n",
    "        self.tr_tokenizer.load(\"../data/tr_spm.model\")\n",
    "\n",
    "\n",
    "    def greedy_decode(self, input_ids, input_key_padding_mask, max_len):\n",
    "        max_len = min(max_len, 50) # just in case there's some crazy sentence out there\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_size = input_ids.size(0)\n",
    "            device = input_ids.device\n",
    "\n",
    "            decoded = torch.full((batch_size, 1), self.BOS_ID, dtype=torch.long, device=device)\n",
    "            finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "            for step in range(max_len):\n",
    "                logits = self.model(input_ids, decoded, src_key_padding_mask=input_key_padding_mask)\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                decoded = torch.cat([decoded, next_token], dim=1)\n",
    "                finished |= next_token.squeeze(1) == self.EOS_ID\n",
    "                if finished.all():\n",
    "                    break\n",
    "\n",
    "            return decoded\n",
    "\n",
    "\n",
    "    def decode_sequence(self, sequence, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tr_tokenizer\n",
    "        tokens = []\n",
    "        for tok in sequence:\n",
    "            if tok == self.EOS_ID:\n",
    "                break\n",
    "            if tok not in {self.BOS_ID, self.PAD_ID}:\n",
    "                tokens.append(tok)\n",
    "        return tokenizer.decode(tokens)\n",
    "\n",
    "    \n",
    "    def compute_bleu(self, reference, hypothesis):\n",
    "        return sacrebleu.sentence_bleu(hypothesis, [reference]).score\n",
    "            \n",
    "\n",
    "    def _run_epoch(self, loader, train=True):\n",
    "        epoch_loss = 0.0\n",
    "        num_nans = 0\n",
    "        bleu_scores = []\n",
    "\n",
    "        last_report = time.time()\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "\n",
    "        loader = tqdm.tqdm(loader, desc=\"Training\" if train else \"Validation\", mininterval=10) # Progress bar, updating every 10 seconds to avoid cluttering the output\n",
    "        for batch in loader:\n",
    "            input_ids, input_key_padding_mask, label_ids, label_key_padding_mask = batch\n",
    "            input_ids, input_key_padding_mask = input_ids.to(self.device, non_blocking=True), input_key_padding_mask.to(self.device, non_blocking=True)\n",
    "            label_ids, label_key_padding_mask = label_ids.to(self.device, non_blocking=True), label_key_padding_mask.to(self.device, non_blocking=True)\n",
    "\n",
    "            if input_key_padding_mask.all(dim=1).any():\n",
    "                print(f\"[!] Skipping batch with all-padding input sequence.\")\n",
    "                continue\n",
    "\n",
    "            if label_key_padding_mask.all(dim=1).any():\n",
    "                print(f\"[!] Skipping batch with all-padding target sequence.\")\n",
    "                continue\n",
    "\n",
    "            assert input_ids.shape == input_key_padding_mask.shape, \"masking error with input_ids\"\n",
    "            assert label_ids.shape == label_key_padding_mask.shape, \"masking error with label_ids\"\n",
    "\n",
    "\n",
    "            if train:\n",
    "                self.optimizer.zero_grad(set_to_none=True) # zero the gradients\n",
    "            \n",
    "                #tgt_input = label_ids[:, :-1] # uncomment to go back to regular training\n",
    "                #targets = label_ids[:, 1:] # uncomment to go back to regular training\n",
    "                #tgt_key_padding_mask = label_key_padding_mask[:, :-1] # uncomment to go back to regular training\n",
    "\n",
    "                ##################################################### EXPERIMENTAL PART #####################################################\n",
    "\n",
    "                raw_tgt = label_ids[:, :-1].clone()                      # copy the target sequence\n",
    "                eos_pos = raw_tgt == self.EOS_ID                         # find the position of the EOS token in the target sequence\n",
    "                raw_tgt[eos_pos] = self.PAD_ID                           # replace the EOS token with the PAD token in the target sequence\n",
    "                tgt_input = raw_tgt                                      # use the modified target sequence as input to the decoder\n",
    "                targets    = label_ids[:, 1:]                            # use the original target sequence as targets for the loss function\n",
    "                tgt_key_padding_mask = label_key_padding_mask[:, :-1] | eos_pos # replace the EOS token with the PAD token in the target sequence\n",
    "\n",
    "                # why do we do this? because the model is trained to predict the EOS token, but we don't want it to see the EOS token during teacher forcing.\n",
    "                # this way, it's still penalized for not predicting EOS in cross entropy loss, but it doesn't see the EOS token during teacher forcing.\n",
    "                # if i didn't do this, my auxiliary loss would simply sit in a local minimum where it stops after it sees an EOS token.\n",
    "\n",
    "                ##################################################### EXPERIMENTAL PART END #####################################################\n",
    "\n",
    "                if self.scaler: # to use mixed precision training\n",
    "                    #with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    with torch.amp.autocast(device_type=\"cuda\", enabled=False): # to disable in case of numerical instability\n",
    "                        logits = self.model(\n",
    "                            input_ids,\n",
    "                            tgt_input,\n",
    "                            src_key_padding_mask=input_key_padding_mask,\n",
    "                            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                        )\n",
    "                        logits = logits.view(-1, logits.size(-1)) # only running the forward pass with autocast for now, when i add backprop it causes NaN errors\n",
    "\n",
    "                        targets = targets.reshape(-1)\n",
    "\n",
    "                    logits = logits.float() # convert logits to float32 for loss calculation so that float16 values don't sneak in and cause NaN errors\n",
    "                    # loss = self.loss_fn(logits, targets) # this is the original line without a custom loss function\n",
    "\n",
    "                    # below is an implementation of a loss function that doubles the loss for eos tokens, so the model is incentivized to stop at the right time.\n",
    "\n",
    "                    losses = F.cross_entropy(logits, targets, reduction=\"none\", ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "                    eos_mask = (targets == self.EOS_ID).float()\n",
    "                    weights = 1.0 + eos_mask  # 2 if EOS, else 1, effectively doubling the loss for EOS tokens\n",
    "                    weighted_losses = losses * weights # EOS tokens get double the loss\n",
    "                    loss = weighted_losses.mean() # mean loss over the batch\n",
    "\n",
    "\n",
    "\n",
    "                    ##################################################### EXPERIMENTAL PART #####################################################\n",
    "\n",
    "                    batch_size = input_ids.size(0)\n",
    "                    logits_reshaped = logits.view(batch_size, -1, logits.size(-1))\n",
    "                    eos_probs = F.softmax(logits_reshaped, dim=-1)[..., self.EOS_ID]\n",
    "\n",
    "                    seq_len =eos_probs.size(1)\n",
    "                    positions = torch.arange(seq_len, device=self.device).float().unsqueeze(0).expand(batch_size, -1)\n",
    "                    true_eos_pos = (label_ids == self.EOS_ID).int().argmax(dim=1).unsqueeze(1) \n",
    "\n",
    "                    ramp_mask = positions < true_eos_pos # positions where the EOS probability should be getting ramped up as we approach the end of the sentence.\n",
    "                    peak_mask = positions == true_eos_pos # the true EOS position\n",
    "                    tail_mask = positions > true_eos_pos # tail of the output, i.e. padded positions\n",
    "\n",
    "                    rel_diffs = (positions - true_eos_pos) / (true_eos_pos + 1e-6)\n",
    "\n",
    "                    ramp = torch.exp(-(rel_diffs ** 2) / (2 * 0.05 ** 2)) * ramp_mask \n",
    "                    peak = peak_mask.float()\n",
    "                    tail = tail_mask.float() # need exactly eos at the padded positions as well, since we don't feed EOS during teacher forcing the model constantly gets the finished sentence it needs to add EOS to.\n",
    "\n",
    "                    target = (ramp + peak + tail) * 0.9 # because of label smoothing, the target probability is 0.9 at the correct positions, not 1.\n",
    "                    target = target / (target.sum(dim=1, keepdim=True) + 1e-8) # final target distribution\n",
    "                    eos_probs = eos_probs / (eos_probs.sum(dim=1, keepdim=True) + 1e-8) # the real distribution is normalized too\n",
    "\n",
    "                    eos_loss = F.kl_div(eos_probs.log(), target, reduction='batchmean').clamp(max=10) # KL divergence calculates a differentiable loss, comparing the divergence between the two distributions\n",
    "\n",
    "                    total_loss = eos_loss + loss\n",
    "                    \n",
    "                    ##################################################### EXPERIMENTAL PART END #####################################################\n",
    "\n",
    "\n",
    "\n",
    "                    #if torch.isnan(loss).any() or torch.isinf(loss).any() or loss.item() > 1e3: unexperimental\n",
    "                    if torch.isnan(total_loss).any() or torch.isinf(total_loss).any() or total_loss.item() > 1e3:\n",
    "                        print(f\"[!] Skipping batch with NaN/Inf/unusually large loss. Input IDs shape: {input_ids.shape}\")\n",
    "                        self.optimizer.zero_grad()\n",
    "                        num_nans += 1\n",
    "                        continue\n",
    "\n",
    "                    #epoch_loss += loss.item()\n",
    "                    epoch_loss += total_loss.item() # uncomment the above line to use the original loss function\n",
    "\n",
    "                    main_loss = loss.item() # for logging purposes, the original loss function\n",
    "                    aux_loss = eos_loss.item() # for logging purposes, the experimental loss function\n",
    "\n",
    "                    self.scaler.scale(total_loss).backward() # Backpropagation\n",
    "\n",
    "                    nan_grad = False\n",
    "                    for name, param in self.model.named_parameters():\n",
    "                        if param.grad is not None:\n",
    "                            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                                print(f\"[!] Skipping batch with NaN/Inf gradient in {name}\")\n",
    "                                num_nans += 1\n",
    "                                \n",
    "                                nan_grad = True\n",
    "                                break\n",
    "                    if nan_grad:\n",
    "                        self.optimizer.zero_grad()\n",
    "                        continue\n",
    "\n",
    "                    if num_nans > 15:\n",
    "                        send_message(content=f\"Too many NaN/Inf gradients. Stopping training.\")\n",
    "                        raise RuntimeError(\"Too many NaN/Inf gradients. Stopping training.\")\n",
    "                    \n",
    "\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5) # gradient clipping, since the model is fairly large\n",
    "\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                    loader.set_postfix(main_loss=main_loss, eos_loss=aux_loss) # Update progress bar\n",
    "                    \n",
    "                else:\n",
    "                    logits = self.model(\n",
    "                        input_ids,\n",
    "                        tgt_input,\n",
    "                        src_key_padding_mask=input_key_padding_mask,\n",
    "                        tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                    )\n",
    "                    logits = logits.view(-1, logits.size(-1)) \n",
    "                    targets = targets.reshape(-1)\n",
    "\n",
    "                    losses = F.cross_entropy(logits, targets, reduction=\"none\", ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "                    eos_mask = (targets == self.EOS_ID).float()\n",
    "                    weights = 1.0 + eos_mask  # 2 if EOS, else 1\n",
    "                    weighted_losses = losses * weights # EOS tokens get double the loss\n",
    "                    loss = weighted_losses.mean() # mean loss over the batch\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                    loss.backward() # Backpropagation\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5) # gradient clipping, since the model is fairly large\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    self.scheduler.step() # update learning rate each step\n",
    "\n",
    "                    loader.set_postfix(loss=loss.item()) # Update progress bar\n",
    "\n",
    "\n",
    "                now = time.time()\n",
    "\n",
    "                if now - last_report >= 10*60: # report every 10 minutes\n",
    "                    send_message(tqdm_obj=loader)\n",
    "                    last_report = now\n",
    "            \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    tgt_input = label_ids[:, :-1]\n",
    "                    targets = label_ids[:, 1:]\n",
    "                    tgt_key_padding_mask = label_key_padding_mask[:, :-1]\n",
    "\n",
    "                    logits = self.model(\n",
    "                        input_ids,\n",
    "                        tgt_input,\n",
    "                        src_key_padding_mask=input_key_padding_mask,\n",
    "                        tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                    )\n",
    "                    logits = logits.view(-1, logits.size(-1))\n",
    "                    targets = targets.reshape(-1)\n",
    "                    \n",
    "                    losses = F.cross_entropy(logits, targets, reduction=\"none\", ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "                    eos_mask = (targets == self.EOS_ID).float()\n",
    "                    weights = 1.0 + eos_mask  # 2 if EOS, else 1\n",
    "                    weighted_losses = losses * weights # EOS tokens get double the loss\n",
    "                    loss = weighted_losses.mean() # mean loss over the batch\n",
    "\n",
    "                    #### BLEU score calculation #####\n",
    "                    if random.random() < 0.025:  # 2.5% of batches\n",
    "                        max_len = (label_ids != 0).sum(dim=1).max().item()  # length cap based on label lengths\n",
    "                        predictions = self.greedy_decode(input_ids, input_key_padding_mask, max_len)\n",
    "\n",
    "                        for i in range(min(len(predictions), 3)):  # limit examples per batch\n",
    "                            pred_text = self.decode_sequence(predictions[i].tolist())\n",
    "                            true_text = self.decode_sequence(label_ids[i].tolist())\n",
    "\n",
    "                            bleu = self.compute_bleu(true_text, pred_text)\n",
    "                            bleu_scores.append(bleu)\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                loader.set_postfix(loss=loss.item())\n",
    "\n",
    "        if len(loader) > 0:\n",
    "            if not train:\n",
    "                avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "                if bleu_scores:\n",
    "                    print(f\"Average BLEU score this epoch: {avg_bleu:.4f}, with {len(bleu_scores)} samples.\")\n",
    "                    send_message(content=f\"üü¢ Validation BLEU: {avg_bleu:.4f} ({len(bleu_scores)} samples)\")\n",
    "                else:\n",
    "                    print(\"[BLEU] No samples chosen this epoch for BLEU calculation.\")\n",
    "                    send_message(content=\"[BLEU] No samples chosen this epoch for BLEU calculation.\")\n",
    "            return epoch_loss / len(loader)\n",
    "        else:\n",
    "            return epoch_loss\n",
    "\n",
    "    def train(self, num_epochs=50, patience=5, verbose=True):\n",
    "        try:\n",
    "            done_steps = self.current_step\n",
    "            \n",
    "            for epoch in range(self.start_epoch, num_epochs):\n",
    "                if self.max_tokens + epoch <= 20:\n",
    "                    max_tok = self.max_tokens + epoch\n",
    "                else:\n",
    "                    max_tok = min((epoch + self.max_tokens - 20)*2 + 20, 35) # faster increase after basic structures are learned\n",
    "                    \n",
    "\n",
    "                train_dataset = DatasetFromCSV(filepath=self.filepath, mode=\"train\", \n",
    "                                               batches_per_size=self.batches_per_size,\n",
    "                                               max_tokens=max_tok, # max length of 40 because my system is trash\n",
    "                                               batch_size=self.batch_size\n",
    "                )\n",
    "\n",
    "                train_loader = DataLoader(\n",
    "                    dataset=train_dataset,\n",
    "                    batch_sampler=BucketBatchSampler(train_dataset.get_lengths(), batch_size=self.batch_size, shuffle=True),\n",
    "                    collate_fn=self.preprocessor,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "\n",
    "                print(f\"Loaded {len(train_loader)} batches of training data. Starting epoch {epoch+1}/{num_epochs}... (Current LR: {self.scheduler.get_last_lr()[0]:.8f})\")\n",
    "                send_message(\n",
    "                    content=(\n",
    "                        f\"üü° Epoch {epoch+1}/{num_epochs} started\\n\"\n",
    "                        f\"üîπ Batches: {len(train_loader)}\\n\"\n",
    "                        f\"üîπ Max Tokens: {max_tok} | Batch Size: {self.batch_size} | Batches/Size: {self.batches_per_size}\\n\"\n",
    "                        f\"üöÄ LR: {self.scheduler.get_last_lr()[0]:.8f}\\n\"\n",
    "                        f\"üîÅ Steps Done: {done_steps}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                train_loss = self._run_epoch(train_loader, train=True)\n",
    "                done_steps += len(train_loader)\n",
    "\n",
    "                del train_loader, train_dataset # clear the memory\n",
    "                torch.cuda.empty_cache() # clear the cache to avoid OOM errors\n",
    "\n",
    "                val_dataset = DatasetFromCSV(filepath=self.val_filepath, mode=\"val\", batches_per_size=self.batches_per_size, batch_size=self.batch_size)\n",
    "\n",
    "                val_loader = DataLoader(\n",
    "                    dataset=val_dataset,\n",
    "                    batch_sampler=BucketBatchSampler(val_dataset.get_lengths(), batch_size=self.batch_size, shuffle=True),\n",
    "                    collate_fn=self.preprocessor,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "\n",
    "                val_loss = self._run_epoch(val_loader, train=False)\n",
    "\n",
    "                current_lr = self.scheduler.get_last_lr()[0]\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "                    print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Current Learning Rate: {current_lr:.8f}\") \n",
    "\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.epochs_without_improvement = 0\n",
    "                    torch.save(self.model.state_dict(), self.save_path)\n",
    "                    if verbose:\n",
    "                        print(f\"Best model saved to {self.save_path}.\")\n",
    "                else:\n",
    "                    self.epochs_without_improvement += 1\n",
    "\n",
    "                if self.epochs_without_improvement >= patience:\n",
    "                    if verbose:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "                del val_loader, val_dataset # clear the memory\n",
    "                torch.cuda.empty_cache() # clear the cache to avoid OOM errors\n",
    "                if verbose:\n",
    "                    print(  f\"üü° Epoch {epoch+1}/{num_epochs} finished\\n\"\n",
    "                            f\"üîπ Max Tokens: {max_tok} | Batch Size: {self.batch_size} | Batches/Size: {self.batches_per_size}\\n\"\n",
    "                            f\"üöÄ LR: {current_lr:.8f}\\n\"\n",
    "                            f\"üîÅ Steps Done: {done_steps}\"\n",
    "                            f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "                            )\n",
    "\n",
    "                send_message(embed=make_embed(epoch+1, train_loss, val_loss, current_lr, done_steps, max_tok, self.batches_per_size))\n",
    "\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user. Saving checkpoint...\")\n",
    "            torch.save(self.model.state_dict(), \"checkpoint.pth\")\n",
    "            if verbose:\n",
    "                print(\"Checkpoint model saved.\")\n",
    "\n",
    "# pad_id=0, unk_id=1, bos_id=2, eos_id=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(self):\n",
    "    for name, module in self.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if name == \"output_projection\":\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from zero:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = Preprocessor(pad_token_id=0)\n",
    "    model = Transformer(vocab_size=50000, n_encoders=5, n_decoders=5, embed_dim=512, ff_dim=2048, num_heads=8, dropout=0.2)\n",
    "    model.apply(init_weights) # initialize weights\n",
    "    optimizer = AdamW(model.parameters(), \n",
    "                    lr=0.0001,\n",
    "                    weight_decay=0.01)\n",
    "\n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        preprocessor=preprocessor,\n",
    "        batch_size=16, # trash gpu\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        save_path=\"best_model.pth\",\n",
    "        max_tokens=3, # starting from a very easy point\n",
    "        batches_per_size=2500, # number of batches per size\n",
    "        num_training_steps=1000000, # total number of training steps\n",
    "        num_warmup_steps=37500, # number of warmup steps for learning rate scheduler\n",
    "        num_plateau_steps=150000, # number of plateau steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "    trainer.train(num_epochs=50, patience=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To continue training the best model so far (from the end of the last completed epoch):\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = Preprocessor(pad_token_id=0)\n",
    "    model = Transformer(vocab_size=50000, n_encoders=5, n_decoders=5, embed_dim=512, ff_dim=2048, num_heads=8, dropout=0.1)\n",
    "    optimizer = AdamW(model.parameters(), \n",
    "                    lr=0.0001,\n",
    "                    weight_decay=0.01)\n",
    "\n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        preprocessor=preprocessor,\n",
    "        batch_size=16, # trash gpu\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        save_path=\"best_model.pth\",\n",
    "        load_model_path=\"best_model.pth\",  # Specify the path to the saved model\n",
    "        max_tokens=3, # starting point\n",
    "        batches_per_size=2500, # number of batches per size\n",
    "        num_training_steps=840000, # total number of training steps\n",
    "        num_warmup_steps=0, # number of warmup steps for learning rate scheduler\n",
    "        num_plateau_steps=40000, # number of plateau steps for learning rate scheduler\n",
    "        current_step=0, # to go back to the correct optimizer state\n",
    "        start_epoch=31, # specify the epoch to start from (0-indexed)\n",
    "    )\n",
    "\n",
    "    trainer.train(num_epochs=75, patience=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to continue from a checkpoint:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = Preprocessor(pad_token_id=0)\n",
    "    model = Transformer(vocab_size=50000, n_encoders=5, n_decoders=5, embed_dim=512, ff_dim=2048, num_heads=8, dropout=0.1)\n",
    "    optimizer = AdamW(model.parameters(), \n",
    "                    lr=0.000094,\n",
    "                    weight_decay=0.01)\n",
    "\n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        preprocessor=preprocessor,\n",
    "        batch_size=16,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        save_path=\"best_model.pth\",\n",
    "        load_model_path=\"checkpoint.pth\",\n",
    "        max_tokens=3, # starting point\n",
    "        batches_per_size=2500, # number of batches per size\n",
    "        num_training_steps=1850000, # total number of training steps\n",
    "        num_warmup_steps=10000, # number of warmup steps for learning rate scheduler\n",
    "        num_plateau_steps=1200000, # number of plateau steps for learning rate scheduler\n",
    "        current_step=1150000, # to go back to the correct optimizer state\n",
    "        start_epoch=30, # specify the epoch to start from (0-indexed)\n",
    "    )\n",
    "\n",
    "    trainer.train(num_epochs=70, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test with fake data\n",
    "\n",
    "def generate_random_sequence(min_length=3, max_length=25, vocab_size=50000):\n",
    "    length = np.random.randint(min_length, max_length + 1)\n",
    "    sequence = \" \".join(map(str, np.random.randint(1, vocab_size, size=length)))\n",
    "    return sequence\n",
    "\n",
    "def generate_fake_data(num_examples=10000, min_length=5, max_length=20, vocab_size=50000):\n",
    "    data = {\n",
    "        \"input_ids\": [\"1 \" + generate_random_sequence(min_length, max_length, vocab_size) + \" 2\" for _ in range(num_examples)]\n",
    "    }\n",
    "    data[\"label_ids\"] = [seq for seq in data[\"input_ids\"]]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "fake_data = generate_fake_data(num_examples=10000, min_length=5, max_length=20, vocab_size=1000)\n",
    "\n",
    "fake_data.to_csv(\"test_small.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor = Preprocessor(pad_token_id=0)\n",
    "    model = Transformer(vocab_size=50000, n_encoders=4, n_decoders=4, embed_dim=512, ff_dim=2048, num_heads=8)\n",
    "    optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        preprocessor=preprocessor,\n",
    "        batch_size=32,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        save_path=\"worse_model.pth\",\n",
    "        val_filepath=\"test_small.csv\", \n",
    "        filepath=\"test_small.csv\",\n",
    "        sample_frac=1\n",
    "    )\n",
    "\n",
    "    trainer.train(num_epochs=50, patience=5, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sentencepiece as spm\n",
    "from transformer_copy import Transformer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferBottle:\n",
    "    def __init__(self, model_path=\"../model/best_model.pth\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\", vocab_size=50000,\n",
    "                 en_tokenizer_path=\"../data/en_spm.model\", tr_tokenizer_path=\"../data/tr_spm.model\", beam_width=5, max_len=500, sampling_method=\"all\"):\n",
    "\n",
    "        sp_en = spm.SentencePieceProcessor()\n",
    "        sp_en.load(en_tokenizer_path)\n",
    "        self.en_tokenizer = sp_en\n",
    "\n",
    "        sp_tr = spm.SentencePieceProcessor()\n",
    "        sp_tr.load(tr_tokenizer_path)\n",
    "        self.tr_tokenizer = sp_tr\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model = Transformer(vocab_size=self.vocab_size, embed_dim=512, ff_dim=2048, num_heads=8, n_encoders=5, n_decoders=5)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.device = device\n",
    "        self.sampling_method = sampling_method\n",
    "        self.beam_width = beam_width\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.BOS_ID = sp_tr.bos_id()\n",
    "        self.EOS_ID = sp_tr.eos_id()\n",
    "        self.PAD_ID = sp_en.pad_id() if sp_en.pad_id() != -1 else 0\n",
    "\n",
    "        assert sampling_method in [\"greedy\", \"top_p\", \"beam_search\", \"all\"], \"Invalid sampling method. Choose one of 'greedy', 'top_p', 'beam_search', or 'all'.\"\n",
    "\n",
    "    def translate(self, text):\n",
    "        input_ids = self.en_tokenizer.encode(text, out_type=int)\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(self.device)\n",
    "        src_key_padding_mask = (input_ids == self.PAD_ID)\n",
    "\n",
    "        def decode_sequence(decoded_ids):\n",
    "            return self.tr_tokenizer.decode(decoded_ids)\n",
    "\n",
    "        if self.sampling_method in [\"greedy\", \"all\"]:\n",
    "            decoded_ids = self._greedy_decode(input_ids, src_key_padding_mask)\n",
    "            greedy_output = decode_sequence(decoded_ids)\n",
    "\n",
    "        if self.sampling_method in [\"top_p\", \"all\"]:\n",
    "            decoded_ids = self._top_p_decode(input_ids, src_key_padding_mask)\n",
    "            top_p_output = decode_sequence(decoded_ids)\n",
    "\n",
    "        if self.sampling_method in [\"beam_search\", \"all\"]:\n",
    "            decoded_ids = self.beam_search(input_ids, src_key_padding_mask)\n",
    "            beam_output = decode_sequence(decoded_ids)\n",
    "\n",
    "        if self.sampling_method == \"greedy\":\n",
    "            return greedy_output\n",
    "        elif self.sampling_method == \"top_p\":\n",
    "            return top_p_output\n",
    "        elif self.sampling_method == \"beam_search\":\n",
    "            return {\n",
    "                \"text\": text,\n",
    "                \"beam_search\": beam_output\n",
    "            }\n",
    "        elif self.sampling_method == \"all\":\n",
    "            return {\n",
    "                \"text\": text,\n",
    "                \"greedy\": greedy_output,\n",
    "                \"top_p\": top_p_output,\n",
    "                \"beam_search\": beam_output\n",
    "            }\n",
    "\n",
    "    def _greedy_decode(self, input_ids, src_key_padding_mask):\n",
    "        decoded_ids = [self.BOS_ID]\n",
    "\n",
    "        repetition_counter = {}\n",
    "        for _ in range(self.max_len):\n",
    "            tgt_input = torch.tensor(decoded_ids).unsqueeze(0).to(self.device)\n",
    "            tgt_key_padding_mask = (tgt_input == 0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(\n",
    "                    input_ids,\n",
    "                    tgt_input,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                )[0, -1, :]\n",
    "\n",
    "            next_token = self.greedy_sampling(logits) # simply taking the argmax over the logits\n",
    "\n",
    "            if next_token == self.EOS_ID:\n",
    "                break\n",
    "\n",
    "            decoded_ids.append(next_token)\n",
    "\n",
    "            repetition_counter[next_token] = repetition_counter.get(next_token, 0) + 1\n",
    "            if repetition_counter[next_token] >= 3:\n",
    "                break\n",
    "\n",
    "        return decoded_ids[1:]\n",
    "\n",
    "    def _top_p_decode(self, input_ids, src_key_padding_mask):\n",
    "        decoded_ids = [self.BOS_ID]\n",
    "\n",
    "        repetition_counter = {}\n",
    "        for _ in range(self.max_len):\n",
    "            tgt_input = torch.tensor(decoded_ids).unsqueeze(0).to(self.device)\n",
    "            tgt_key_padding_mask = (tgt_input == 0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(\n",
    "                    input_ids,\n",
    "                    tgt_input,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                )[0, -1, :]\n",
    "\n",
    "            for token in set(decoded_ids):  # to discourage repetition, can be disabled for more advanced models\n",
    "                logits[token] /= 1.2\n",
    "\n",
    "            next_token = self.top_p_sample(logits, p=0.9, temperature=0.8) # random sampling from the top-p cumulative probability mass (0.9)\n",
    "            if next_token == self.EOS_ID:\n",
    "                break\n",
    "            decoded_ids.append(next_token)\n",
    "\n",
    "            repetition_counter[next_token] = repetition_counter.get(next_token, 0) + 1\n",
    "            if repetition_counter[next_token] >= 3:\n",
    "                break\n",
    "\n",
    "        return decoded_ids[1:]\n",
    "\n",
    "    def beam_search(self, input_ids, src_key_padding_mask):\n",
    "        beams = [([self.BOS_ID], 0.0, False)]\n",
    "        for _ in range(self.max_len):\n",
    "            candidates = []\n",
    "\n",
    "            for seq, score, has_ended in beams:\n",
    "                if has_ended:\n",
    "                    candidates.append((seq, score, True))\n",
    "                    continue\n",
    "\n",
    "                tgt_input = torch.tensor(seq).unsqueeze(0).to(self.device)\n",
    "                tgt_key_padding_mask = (tgt_input == 0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(\n",
    "                        input_ids,\n",
    "                        tgt_input,\n",
    "                        src_key_padding_mask=src_key_padding_mask,\n",
    "                        tgt_key_padding_mask=tgt_key_padding_mask\n",
    "                    )[0, -1, :]\n",
    "\n",
    "                for token in set(seq):\n",
    "                    logits[token] /= 1.3  # reduce the probability of already generated tokens\n",
    "\n",
    "                probs = F.log_softmax(logits, dim=-1) # log softmax for numerical stability\n",
    "                topk_probs, topk_indices = torch.topk(probs, self.beam_width)\n",
    "\n",
    "                for log_prob, token_id in zip(topk_probs.tolist(), topk_indices.tolist()):\n",
    "                    new_seq = seq + [token_id]\n",
    "                    new_score = score + log_prob\n",
    "                    ended = token_id == self.EOS_ID\n",
    "                    candidates.append((new_seq, new_score, ended))\n",
    "\n",
    "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:self.beam_width] # keep the top-beam_width sequences\n",
    "\n",
    "            if all(ended for _, _, ended in beams): # break if all sequences have ended\n",
    "                break\n",
    "\n",
    "        best_seq = beams[0][0]\n",
    "        if best_seq[0] == self.BOS_ID:\n",
    "            best_seq = best_seq[1:]\n",
    "        if best_seq and best_seq[-1] == self.EOS_ID:\n",
    "            best_seq = best_seq[:-1]\n",
    "        return best_seq\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_sampling(logits):\n",
    "        return torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def top_p_sample(logits, p=0.9, temperature=1.0):\n",
    "        logits = logits / temperature # temperature scaling to control randomness/exploration\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        sorted_mask = cumulative_probs > p\n",
    "        sorted_mask[..., 1:] = sorted_mask[..., :-1].clone() # shift the mask to the right to include the first token where cumulative_probs >= p\n",
    "        sorted_mask[..., 0] = False # to always keep the first token\n",
    "\n",
    "        sorted_probs[sorted_mask] = 0\n",
    "        sorted_probs /= sorted_probs.sum() # normalizing the probabilities\n",
    "\n",
    "        next_token = sorted_indices[torch.multinomial(sorted_probs, 1)] # randomly sampling from the distribution\n",
    "        return next_token.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = InferBottle(model_path=\"../model/best_model.pth\", sampling_method=\"beam_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: I’m tired, but I can’t sleep.\n",
      "beam_search: Ben yorgun, ama uyuya Uyamıyorum.\n",
      "\n",
      "\n",
      "text: She left the window open, and now the whole room is freezing.\n",
      "beam_search: Pencere açık bıraktı ve şimdi bütün oda donma. O, o da onu terk\n",
      "\n",
      "\n",
      "text: We don’t always get what we want, but we learn to live with it.\n",
      "beam_search: Her zaman istediğimizi almıyoruz, ama onunla yaşamayı öğreniyoruz.\n",
      "\n",
      "\n",
      "text: Even after all these years, he still remembered the smell of her perfume.\n",
      "beam_search: Tüm bu yıl sonra, hala onun parfüm kokusunu hatırdı.\n",
      "\n",
      "\n",
      "text: The scientist paused, uncertain whether to publish results that might change everything.\n",
      "beam_search: bilim adamı, her şeyi değiştirebilecek sonuçların yayınlanıp yayımlanmaya durakladı.\n",
      "\n",
      "\n",
      "text: They told him it was impossible, but he did it anyway — and now the world knows his name.\n",
      "beam_search: Ona imkansız olduğunu söylediler, ama yine de bunu yaptı ve şimdi dünya adını biliyor.\n",
      "\n",
      "\n",
      "text: In the silence that followed, no one dared to speak.\n",
      "beam_search: Takip eden sessizlik, hiç kimse konuşmaya baktı.\n",
      "\n",
      "\n",
      "text: They couldn't believe their eyes. What they saw was unimaginable.\n",
      "beam_search: Onlar onların gözlerine inanamadılar. Onların ne görünce şaşırmışlardı. Ne gördüler\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = infer.translate(\"I’m tired, but I can’t sleep.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"She left the window open, and now the whole room is freezing.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"We don’t always get what we want, but we learn to live with it.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"Even after all these years, he still remembered the smell of her perfume.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"The scientist paused, uncertain whether to publish results that might change everything.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"They told him it was impossible, but he did it anyway — and now the world knows his name.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"In the silence that followed, no one dared to speak.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"They couldn't believe their eyes. What they saw was unimaginable.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: I wanted to leave, but I stayed.\n",
      "beam_search: Ben ayrılmak istedim, ama ben kaldım Kaldım.\n",
      "\n",
      "\n",
      "text: After the meeting ended, everyone left the room.\n",
      "beam_search: toplantı sona erdi, herkes odadan ayrıldıktan sonra gitti. Toplantı bittiğinde herkesi odadan ayrıldı\n",
      "\n",
      "\n",
      "text: If you had told me earlier, I would have helped.\n",
      "beam_search: Bana bana daha önce bana daha erken söylersen, yardım ederdim. Yardım olurdu.\n",
      "\n",
      "\n",
      "text: The results were announced yesterday.\n",
      "beam_search: sonuçları dün açıklandı. Sonuçlar duyuruldu.\n",
      "\n",
      "\n",
      "text: She turned a blind eye to the whole situation.\n",
      "beam_search: O bütün duruma göz kör bir göze döndü. O, o Gözü tamamen gitti\n",
      "\n",
      "\n",
      "text: The book that he recommended was surprisingly good.\n",
      "beam_search: Onun tavsiye ettiği kitap şaşırtıcı bir şekilde iyiydi. Tavsiye eden o şiddetle tavsiye ederim\n",
      "\n",
      "\n",
      "text: They said he might be coming, but they weren’t sure.\n",
      "beam_search: Onlar geliyor olabileceğini söyledi, ama emin değil dediler.\n",
      "\n",
      "\n",
      "text: It’s hard to say whether what he did was right or wrong.\n",
      "beam_search: Ne yaptığının doğru veya yanlış olup olmadığını söylemek zor olurdu.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = infer.translate(\"I wanted to leave, but I stayed.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"After the meeting ended, everyone left the room.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"If you had told me earlier, I would have helped.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"The results were announced yesterday.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"She turned a blind eye to the whole situation.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"The book that he recommended was surprisingly good.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"They said he might be coming, but they weren’t sure.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "outputs = infer.translate(\"It’s hard to say whether what he did was right or wrong.\")\n",
    "for method, text in outputs.items():\n",
    "    print(f\"{method}: {text}\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

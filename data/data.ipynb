{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "import tarfile\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLITRReader(torch.utils.data.Dataset): # from the original NLI-TR repository\n",
    "    def __init__(self, dataset_name, main_dataset, split_name, max_example_num=-1):\n",
    "        self.dataset_tr = load_dataset(\"nli_tr\", dataset_name, split=split_name, trust_remote_code=True)\n",
    "        self.dataset_en = load_dataset(main_dataset, split=split_name)\n",
    "        self.max_example_num = max_example_num\n",
    "\n",
    "    def read(self):\n",
    "        count = 0\n",
    "        for example_tr, example_en in zip(self.dataset_tr, self.dataset_en):\n",
    "            if example_tr[\"label\"] == -1:  # skip examples having no gold value.\n",
    "                continue\n",
    "            count += 1\n",
    "            if self.max_example_num > 0 and count >= self.max_example_num:\n",
    "                break\n",
    "            yield example_tr, example_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for i in [\"train\", \"validation\", \"test\"]:\n",
    "    try:\n",
    "        reader = NLITRReader(\"snli_tr\", \"snli\", i)\n",
    "        ex = list(reader.read())\n",
    "        examples.extend(ex)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569033"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"canbingol/translate_dataset\")\n",
    "len(ds[\"train\"]) + len(ds[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLITRReader2(torch.utils.data.Dataset): # from the original NLI-TR repository\n",
    "    def __init__(self, main_dataset, split_name, file_path, max_example_num=-1):\n",
    "        self.dataset_tr = pd.read_json(file_path)\n",
    "        self.dataset_en = load_dataset(main_dataset, split=split_name)\n",
    "        self.max_example_num = max_example_num\n",
    "\n",
    "        self.en_dict = {example['pairID']: example for example in self.dataset_en}\n",
    "\n",
    "    def read(self):\n",
    "        for example_tr in self.dataset_tr.itertuples():\n",
    "            pair_id_tr = example_tr.pairID\n",
    "            \n",
    "            if pair_id_tr in self.en_dict:\n",
    "                example_en = self.en_dict[pair_id_tr]\n",
    "\n",
    "                yield example_tr.sentence1, example_en[\"premise\"]\n",
    "                yield example_tr.sentence2, example_en[\"hypothesis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples2 = []\n",
    "\n",
    "for j in [(\"validation_matched\", \"multinli_tr/multinli_tr_1.1_dev_matched.json\"), (\"validation_mismatched\", \"multinli_tr/multinli_tr_1.1_dev_mismatched.json\"), (\"train\", \"multinli_tr/multinli_tr_1.1_train.json\")]:\n",
    "    try:\n",
    "        reader = NLITRReader2(\"nyu-mll/multi_nli\", j[0], j[1])\n",
    "        ex = list(reader.read())\n",
    "        examples2.extend(ex)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for ({j}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824698"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORDS = 1\n",
    "\n",
    "data_label_pairs = []\n",
    "\n",
    "sentences = {}\n",
    "with tarfile.open(\"tatoeba/sentences.tar.bz2\", \"r:bz2\") as tar:\n",
    "    csv_name = [m.name for m in tar.getmembers() if m.isfile()][0]\n",
    "    with tar.extractfile(csv_name) as f:\n",
    "        for line in f:\n",
    "            parts = line.decode(\"utf-8\").strip().split('\\t')\n",
    "            if len(parts) >= 3 and parts[1] in {\"eng\", \"tur\"}:\n",
    "                sentences[parts[0]] = {\"text\": parts[2], \"lang\": parts[1]}\n",
    "\n",
    "\n",
    "\n",
    "pairs = []\n",
    "with tarfile.open(\"tatoeba/links.tar.bz2\", \"r:bz2\") as tar:\n",
    "    csv_name = [m.name for m in tar.getmembers() if m.isfile()][0]\n",
    "    with tar.extractfile(csv_name) as f:\n",
    "        for line in f:\n",
    "            id1, id2 = line.decode(\"utf-8\").strip().split(\"\\t\")\n",
    "            if id1 in sentences and id2 in sentences:\n",
    "                s1 = sentences[id1]\n",
    "                s2 = sentences[id2]\n",
    "                \n",
    "                if {s1[\"lang\"], s2[\"lang\"]} == {\"eng\", \"tur\"}:\n",
    "                    words1 = len(s1[\"text\"].split())\n",
    "                    words2 = len(s2[\"text\"].split())\n",
    "                    \n",
    "                    if words1 >= MIN_WORDS:\n",
    "                        if s1[\"lang\"] == \"eng\":\n",
    "                            pairs.append((s1[\"text\"], s2[\"text\"]))\n",
    "                        else:\n",
    "                            pairs.append((s2[\"text\"], s1[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424304"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists - using cached version\n"
     ]
    }
   ],
   "source": [
    "url = \"https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/moses/en-tr.txt.zip\"\n",
    "zip_path = \"opus-wikipedia/en-tr.txt.zip\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"File already exists - using cached version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159979"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_pairs = []\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "    with zip_file.open(\"Wikipedia.en-tr.en\") as en_file, zip_file.open(\"Wikipedia.en-tr.tr\") as tr_file:\n",
    "        for en_line, tr_line in zip(en_file, tr_file):\n",
    "            try:\n",
    "                en_text = en_line.decode(\"utf-8\").strip()\n",
    "                tr_text = tr_line.decode(\"utf-8\").strip()\n",
    "\n",
    "                wiki_pairs.append((en_text, tr_text))\n",
    "                \n",
    "            except UnicodeDecodeError:\n",
    "                continue  # skip malformed lines\n",
    "\n",
    "len(wiki_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists - using cached version\n"
     ]
    }
   ],
   "source": [
    "url = \"https://object.pouta.csc.fi/OPUS-MultiHPLT/v2/moses/en-tr.txt.zip\"\n",
    "zip_path = \"opus-multihplt/en-tr.txt.zip\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"File already exists - using cached version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21616652"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihplt_pairs = []\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "    with zip_file.open(\"MultiHPLT.en-tr.en\") as en_file, zip_file.open(\"MultiHPLT.en-tr.tr\") as tr_file:\n",
    "        for en_line, tr_line in zip(en_file, tr_file):\n",
    "            try:\n",
    "                en_text = en_line.decode(\"utf-8\").strip()\n",
    "                tr_text = tr_line.decode(\"utf-8\").strip()\n",
    "\n",
    "                en_text = re.sub(r'<[^>]+>', '', en_text)  # remove HTML tags\n",
    "                en_text = re.sub(r'\\s+', ' ', en_text) # normalize whitespace\n",
    "                \n",
    "                tr_text = re.sub(r'<[^>]+>', '', tr_text)\n",
    "                tr_text = re.sub(r'\\s+', ' ', tr_text)\n",
    "                \n",
    "                # Apply length filter\n",
    "                if len(en_text.split()) >= MIN_WORDS and len(tr_text.split()) >= MIN_WORDS:\n",
    "                    multihplt_pairs.append((en_text, tr_text))\n",
    "                    \n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Skip malformed lines\n",
    "\n",
    "len(multihplt_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5049"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_pairs = []\n",
    "\n",
    "with open(\"hand_translated/hand_translated.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            parts = line.strip().split(\",\")\n",
    "            if len(parts) == 2:\n",
    "                eng = parts[0].strip()\n",
    "                tur = parts[1].strip()\n",
    "\n",
    "                hand_pairs.append([eng, tur])\n",
    "\n",
    "                # Capitalized (first letter of each word)\n",
    "                eng_cap = \" \".join([w[0].upper() + w[1:] if w else \"\" for w in eng.split()])\n",
    "                tur_cap = \" \".join([w[0].upper() + w[1:] if w else \"\" for w in tur.split()])\n",
    "\n",
    "                if eng_cap != eng or tur_cap != tur:\n",
    "                    hand_pairs.append([eng_cap, tur_cap])\n",
    "        except Exception as e:\n",
    "            print(\"Error reading line:\", e)\n",
    "            continue\n",
    "\n",
    "len(hand_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'idx': 551, 'premise': 'Beyaz bir bisiklet sokak tabelasına bağlanır.', 'hypothesis': 'bisiklet bir işaretti bağlı', 'label': 0}, {'premise': 'A white bike is tied to a street sign.', 'hypothesis': 'the bike is tied to a sign', 'label': 0})\n",
      "('Bunu söylediğimi unutabileceğini sanmıyorum.', \"'I don't suppose you could forget I ever said that?'\")\n",
      "{'id': [550], 'translation': [{'en': 'Evangeline Lilly is Canadian.', 'tr': 'Evangeline Lilly  Kanadalıdır.'}]}\n",
      "(\"I'm undressing.\", 'Ben soyunuyorum.')\n",
      "('Aşağıkurudere is a village in the District of Emirdağ, Afyonkarahisar Province, Turkey.', 'Aşağıkurudere (eski adı Petera), Afyonkarahisar ilinin Emirdağ ilçesine bağlı bir köydür.')\n",
      "('SHORTSEAPROMOTION CENTER TURKEY ACTION PLAN', 'KISA MESAFE DENİZ TAŞIMACILITANITIM MERKEZİ – TÜRKİYE EYLEM PLANI')\n",
      "['during', 'sırasında']\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.randint(0, 999, 1):\n",
    "    print(examples[i])\n",
    "    print(examples2[i])\n",
    "    print(ds[\"train\"][i:i+1])\n",
    "    print(pairs[i])\n",
    "    print(wiki_pairs[i])\n",
    "    print(multihplt_pairs[i])\n",
    "    print(hand_pairs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label_pairs = []\n",
    "\n",
    "for example_tr, example_en in examples:\n",
    "    input_text_premise = example_en[\"premise\"] # pair the premises and hypotheses\n",
    "    label_text_premise = example_tr[\"premise\"]\n",
    "    data_label_pairs.append((input_text_premise, label_text_premise))\n",
    "    \n",
    "    input_text_hypothesis = example_en['hypothesis']\n",
    "    label_text_hypothesis = example_tr['hypothesis']\n",
    "    data_label_pairs.append((input_text_hypothesis, label_text_hypothesis))\n",
    "\n",
    "for example_tr, example_en in examples2:\n",
    "    data_label_pairs.append((example_en, example_tr))\n",
    "\n",
    "for split in [\"train\", \"validation\"]:    \n",
    "    for entry in ds[\"train\"]:\n",
    "        en_tr = entry[\"translation\"][\"en\"]\n",
    "        tr_tr = entry[\"translation\"][\"tr\"]\n",
    "        data_label_pairs.append((en_tr, tr_tr))\n",
    "\n",
    "#for en_text, tr_text in pairs:\n",
    "#    data_label_pairs.append((en_text, tr_text))\n",
    "data_label_pairs += pairs\n",
    "\n",
    "#for en_text, tr_text in wiki_pairs:\n",
    "#    data_label_pairs.append((en_text, tr_text))\n",
    "data_label_pairs += wiki_pairs\n",
    "data_label_pairs += multihplt_pairs\n",
    "data_label_pairs += hand_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_label_pairs, columns=[\"input_text\", \"label_text\"])\n",
    "df = df.drop_duplicates().reset_index(drop=True) # remove duplicates since the same premise is repeated multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input_text  \\\n",
      "0  A person on a horse jumps over a broken down airplane.   \n",
      "1       A person is training his horse for a competition.   \n",
      "2           A person is at a diner, ordering an omelette.   \n",
      "3                       A person is outdoors, on a horse.   \n",
      "4                   Children smiling and waving at camera   \n",
      "\n",
      "                                             label_text  \n",
      "0    Attaki bir kişi, bozuk bir uçağın üzerinden atlar.  \n",
      "1                 Bir kişi atını yarışma için eğitiyor.  \n",
      "2          Bir kişi bir lokantada omlet sipariş ediyor.  \n",
      "3                    Bir kişi açık havada, at üzerinde.  \n",
      "4  Fotoğraf makinesinde gülümseyen ve sallayan çocuklar  \n",
      "23700223\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           input_text   label_text\n",
      "23700218  Don't Worry  Endişelenme\n",
      "23700219    worry not  endişelenme\n",
      "23700220    Worry Not  Endişelenme\n",
      "23700221      big boy  büyük çocuk\n",
      "23700222      Big Boy  Büyük Çocuk\n"
     ]
    }
   ],
   "source": [
    "print(df[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_sum.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_sum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23700223"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.astype(str)\n",
    "len(df) # 23695835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"input_text\"].to_csv(\"english_corpus.txt\", index=False, header=False)\n",
    "df[\"label_text\"].to_csv(\"turkish_corpus.txt\", index=False, header=False) # save the English and Turkish corpus for training the SentencePiece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the sentencepiece model. We use byte-pair encoding with 50000 tokens for both languages.\n",
    "# The character coverage is set to 0.9999 to ensure that most characters are included in the vocabulary, while hopefully filtering out some junk.\n",
    "# These are hyperparameters that can be tuned.\n",
    "\n",
    "#spm.SentencePieceTrainer.train(input='english_corpus.txt', model_prefix='en_spm', vocab_size=50000, character_coverage=0.9999, model_type=\"bpe\", pad_id=0, unk_id=1, bos_id=2, eos_id=3)\n",
    "#spm.SentencePieceTrainer.train(input='turkish_corpus.txt', model_prefix='tr_spm', vocab_size=50000, character_coverage=0.9999, model_type=\"bpe\", pad_id=0, unk_id=1, bos_id=2, eos_id=3)\n",
    "\n",
    "# commenting out so i dont accidentally overwrite the tokenizer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer = spm.SentencePieceProcessor()\n",
    "tr_tokenizer = spm.SentencePieceProcessor()\n",
    "\n",
    "en_tokenizer.load(\"en_spm.model\")\n",
    "tr_tokenizer.load(\"tr_spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing input_text: 100%|██████████| 23701/23701 [20:43<00:00, 19.06it/s]\n",
      "Processing label_text: 100%|██████████| 23701/23701 [22:38<00:00, 17.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_column_memory_efficient(df, col_name, tokenizer, output_file, chunk_size=1000):\n",
    "    if os.path.exists(output_file):\n",
    "        return\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    with open(output_file, 'wb') as f:\n",
    "        for start_idx in tqdm(range(0, total_rows, chunk_size), \n",
    "                          desc=f\"Processing {col_name}\"):\n",
    "            end_idx = min(start_idx + chunk_size, total_rows)\n",
    "            chunk = df[col_name].iloc[start_idx:end_idx]\n",
    "            \n",
    "            chunk_results = []\n",
    "            for text in chunk:\n",
    "                ids = tokenizer.encode(text, out_type=int)\n",
    "                chunk_results.append([tokenizer.bos_id()] + ids + [tokenizer.eos_id()])\n",
    "            \n",
    "            pickle.dump(chunk_results, f)\n",
    "\n",
    "process_column_memory_efficient(df, \"input_text\", en_tokenizer, \"input_ids.pkl\")\n",
    "process_column_memory_efficient(df, \"label_text\", tr_tokenizer, \"label_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing CSV:  96%|█████████▌| 1.85G/1.93G [06:57<00:18, 4.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "def save_to_readable_csv(input_pkl, label_pkl, output_csv):\n",
    "    with open(input_pkl, 'rb') as f1, open(label_pkl, 'rb') as f2:\n",
    "        def input_generator():\n",
    "            while True:\n",
    "                try:\n",
    "                    yield from pickle.load(f1)\n",
    "                except EOFError:\n",
    "                    break\n",
    "                    \n",
    "        def label_generator():\n",
    "            while True:\n",
    "                try:\n",
    "                    yield from pickle.load(f2)\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"input_ids\", \"label_ids\"])\n",
    "            \n",
    "            f1.seek(0, 2)\n",
    "            f2.seek(0, 2)\n",
    "            total_size = max(f1.tell(), f2.tell())\n",
    "            f1.seek(0)\n",
    "            f2.seek(0)\n",
    "            \n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Writing CSV\") as pbar:\n",
    "                for inp, lbl in zip(input_generator(), label_generator()):\n",
    "                    writer.writerow([\n",
    "                        ' '.join(map(str, inp)),\n",
    "                        ' '.join(map(str, lbl))\n",
    "                    ])\n",
    "                    pbar.update(f1.tell() - pbar.n)\n",
    "\n",
    "save_to_readable_csv(\"input_ids.pkl\", \"label_ids.pkl\", \"tokenized_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>Attaki bir kişi, bozuk bir uçağın üzerinden at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>Bir kişi atını yarışma için eğitiyor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>Bir kişi bir lokantada omlet sipariş ediyor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>Bir kişi açık havada, at üzerinde.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>Fotoğraf makinesinde gülümseyen ve sallayan ço...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person is training his horse for a competition.   \n",
       "2      A person is at a diner, ordering an omelette.   \n",
       "3                  A person is outdoors, on a horse.   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                          label_text  \n",
       "0  Attaki bir kişi, bozuk bir uçağın üzerinden at...  \n",
       "1              Bir kişi atını yarışma için eğitiyor.  \n",
       "2       Bir kişi bir lokantada omlet sipariş ediyor.  \n",
       "3                 Bir kişi açık havada, at üzerinde.  \n",
       "4  Fotoğraf makinesinde gülümseyen ve sallayan ço...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                       in  \\\n",
      "1000005  Designing the wooden terrace was an architectural nightmare because of how high up it had to be.   \n",
      "1000006                                           The conversation was about repainting her kitchen blue.   \n",
      "1000007                                Four bronze horses were placed by the basilica over 100 years ago.   \n",
      "1000008                                                                   I favor the second explanation.   \n",
      "1000009                                                                  The first explanation is better.   \n",
      "\n",
      "                                                                                                        input_ids  \\\n",
      "1000005  [2, 36928, 11, 6960, 4082, 297, 122, 11878, 40581, 1350, 35, 677, 550, 320, 149, 1035, 39, 93, 49548, 3]   \n",
      "1000006                                      [2, 129, 8126, 297, 367, 1130, 171, 585, 1032, 2633, 3467, 49548, 3]   \n",
      "1000007                        [2, 6309, 17089, 15407, 751, 2989, 170, 11, 29789, 568, 1433, 852, 2469, 49548, 3]   \n",
      "1000008                                                                  [2, 80, 3524, 11, 1525, 15916, 49548, 3]   \n",
      "1000009                                                                  [2, 129, 628, 15916, 62, 1702, 49548, 3]   \n",
      "\n",
      "                                                                                        out  \\\n",
      "1000005  Ahşap terası tasarlamak, ne kadar yüksek olması gerektiği için mimari bir kabustu.   \n",
      "1000006                                      Konuşma, mutfağını maviye boyamakla ilgiliydi.   \n",
      "1000007                      Dört bronz at, 100 yıl önce bazilika tarafından yerleştirildi.   \n",
      "1000008                                                      İkinci açıklamayı desteklerim.   \n",
      "1000009                                                           İlk açıklama daha iyidir.   \n",
      "\n",
      "                                                                                                   label_ids  \n",
      "1000005  [2, 14083, 6971, 23258, 49850, 795, 527, 604, 1452, 7722, 71, 5930, 48, 543, 2148, 49840, 49848, 3]  \n",
      "1000006                               [2, 17750, 49850, 42433, 24961, 318, 4088, 16186, 587, 5624, 49848, 3]  \n",
      "1000007                       [2, 8673, 15462, 786, 49850, 1759, 314, 755, 40742, 422, 3407, 1586, 49848, 3]  \n",
      "1000008                                                                 [2, 8132, 32678, 4863, 49, 49848, 3]  \n",
      "1000009                                                                 [2, 3009, 1865, 275, 9681, 49848, 3]  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"tokenized_data.csv\")[1000005:1000010]\n",
    "\n",
    "def str_to_ids(s):\n",
    "    return [int(x) for x in s.split()]\n",
    "\n",
    "df[\"input_ids\"] = df[\"input_ids\"].apply(str_to_ids)\n",
    "df[\"label_ids\"] = df[\"label_ids\"].apply(str_to_ids)\n",
    "\n",
    "n = 5\n",
    "df[\"in\"] = df[\"input_ids\"].apply(lambda ids: en_tokenizer.decode(ids))\n",
    "df[\"out\"] = df[\"label_ids\"].apply(lambda ids: tr_tokenizer.decode(ids))\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df[[\"in\", \"input_ids\", \"out\", \"label_ids\"]].head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tokenized_data.csv\")\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.02) # more than enough with the number of samples we have.\n",
    "df_val.to_csv(\"tokenized_data_val.csv\", index=False)\n",
    "df_train.to_csv(\"tokenized_data_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474005 23226218\n"
     ]
    }
   ],
   "source": [
    "print(len(df_val), len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the CSV from tokenized_data_train.csv...\n",
      "Shuffling...\n",
      "Saving shuffled CSV to tokenized_data_train.csv...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def shuffle_csv(input_path, output_path, seed=42):\n",
    "    print(f\"Reading the CSV from {input_path}...\")\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    print(\"Shuffling...\")\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Saving shuffled CSV to {output_path}...\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "shuffle_csv(\"tokenized_data_train.csv\", \"tokenized_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23226218"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tokenized_data_train.csv\")\n",
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
